"""
Multi-Model Embedding Service

Sentence-Transformers + BGE-M3 í…ìŠ¤íŠ¸ ì„ë² ë”© ì„œë¹„ìŠ¤

ì£¼ìš” ê¸°ëŠ¥:
1. /embed: Sentence-Transformers ì„ë² ë”©
2. /embed_bge_m3: BGE-M3 ì„ë² ë”©
3. /similarity: ìœ ì‚¬ë„ ê³„ì‚°
4. /health: í—¬ìŠ¤ì²´í¬
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import logging
import os

# BGE-M3ìš© imports
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

# ============================================================================
# ë¡œê¹… ì„¤ì •
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# BGE-M3 Embedder í´ë˜ìŠ¤
# ============================================================================

class Embedder:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (Hugging Face Transformers ì§ì ‘ ì‚¬ìš©)"""
    
    _model: Optional[AutoModel] = None
    _tokenizer: Optional[AutoTokenizer] = None
    _device: str = None
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_model()
    
    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if self._model is None:
            print(f"Loading embedding model: {self.model_name}")
            print(f"Device: {self._device}")
            
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self._model = AutoModel.from_pretrained(self.model_name)
            self._model.to(self._device)
            self._model.eval()
            
            print(f"Embedding model loaded")
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling - í† í° ì„ë² ë”©ì˜ í‰ê· """
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    async def embed(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        if not text:
            return []
        
        with torch.no_grad():
            # í† í°í™”
            encoded_input = self._tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(self._device)
            
            # ëª¨ë¸ ì‹¤í–‰
            model_output = self._model(**encoded_input)
            
            # Mean pooling
            embedding = self._mean_pooling(model_output, encoded_input['attention_mask'])
            
            # ì •ê·œí™”
            embedding = F.normalize(embedding, p=2, dim=1)
            
            return embedding.cpu().numpy()[0].tolist()
    
    async def embed_batch(self, texts: List[str], batch_size: int = 32, normalize: bool = True) -> List[List[float]]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            normalize: ì •ê·œí™” ì—¬ë¶€
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_texts = [t for t in texts if t]
        if not valid_texts:
            return []
        
        print(f"ğŸ”¢ Embedding {len(valid_texts)} texts...")
        
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(valid_texts), batch_size):
                batch_texts = valid_texts[i:i + batch_size]
                
                # í† í°í™”
                encoded_input = self._tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                ).to(self._device)
                
                # ëª¨ë¸ ì‹¤í–‰
                model_output = self._model(**encoded_input)
                
                # Mean pooling
                embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
                
                # ì •ê·œí™”
                if normalize:
                    embeddings = F.normalize(embeddings, p=2, dim=1)
                
                all_embeddings.append(embeddings.cpu().numpy())
                
                if (i + batch_size) % (batch_size * 10) == 0:
                    print(f"  Progress: {min(i + batch_size, len(valid_texts))}/{len(valid_texts)}")
        
        # Concatenate all batches
        all_embeddings = np.vstack(all_embeddings)
        
        print(f"Embeddings generated")
        return all_embeddings.tolist()
    
    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        # ë”ë¯¸ í…ìŠ¤íŠ¸ë¡œ ì°¨ì› í™•ì¸
        with torch.no_grad():
            encoded_input = self._tokenizer(
                "test",
                padding=True,
                truncation=True,
                return_tensors='pt'
            ).to(self._device)
            
            model_output = self._model(**encoded_input)
            embedding = self._mean_pooling(model_output, encoded_input['attention_mask'])
            
            return embedding.shape[1]

# ============================================================================
# FastAPI ì•± ì´ˆê¸°í™”
# ============================================================================
app = FastAPI(
    title="Multi-Model Embedding Service",
    description="Sentence-BERT + BGE-M3 í…ìŠ¤íŠ¸ ì„ë² ë”© ì„œë¹„ìŠ¤",
    version="1.0.0"
)

# ============================================================================
# ì „ì—­ ë³€ìˆ˜: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
# ============================================================================
# ê¸°ì¡´ Sentence-Transformers ëª¨ë¸
model: Optional[SentenceTransformer] = None
MODEL_NAME = os.getenv(
    "MODEL_NAME", 
    "sentence-transformers/distiluse-base-multilingual-cased-v2"
)

# ìƒˆë¡œ ì¶”ê°€: BGE-M3 ëª¨ë¸
bge_embedder: Optional[Embedder] = None
BGE_MODEL_NAME = "BAAI/bge-m3"

# ============================================================================
# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜
# ============================================================================

class EmbedRequest(BaseModel):
    """ì„ë² ë”© ìƒì„± ìš”ì²­"""
    texts: List[str] = Field(..., description="ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸", min_items=1)
    normalize: bool = Field(True, description="ì„ë² ë”© ì •ê·œí™” ì—¬ë¶€")

class EmbedResponse(BaseModel):
    """ì„ë² ë”© ìƒì„± ì‘ë‹µ"""
    embeddings: List[List[float]] = Field(..., description="ìƒì„±ëœ ì„ë² ë”©")
    dimension: int = Field(..., description="ì„ë² ë”© ì°¨ì›")
    count: int = Field(..., description="ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ìˆ˜")

class SimilarityRequest(BaseModel):
    """ìœ ì‚¬ë„ ê³„ì‚° ìš”ì²­"""
    query_text: str = Field(..., description="ì¿¼ë¦¬ í…ìŠ¤íŠ¸")
    corpus_embeddings: List[List[float]] = Field(..., description="ì½”í¼ìŠ¤ ì„ë² ë”©")

class SimilarityResponse(BaseModel):
    """ìœ ì‚¬ë„ ê³„ì‚° ì‘ë‹µ"""
    similarities: List[float] = Field(..., description="ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸")
    count: int = Field(..., description="ê³„ì‚°ëœ ìœ ì‚¬ë„ ìˆ˜")

# ============================================================================
# ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
# ============================================================================

@app.on_event("startup")
async def load_model():
    """ì•± ì‹œì‘ ì‹œ ë‘ ê°œì˜ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œ"""
    global model, bge_embedder
    
    try:
        # 1. ê¸°ì¡´ Sentence-Transformers ëª¨ë¸ ë¡œë“œ
        logger.info(f"ğŸ“¦ Sentence-Transformers ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_NAME}")
        model = SentenceTransformer(MODEL_NAME)
        logger.info(f"âœ… Sentence-Transformers ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        logger.info(f"   ì°¨ì›: {model.get_sentence_embedding_dimension()}")
        
        # 2. BGE-M3 ëª¨ë¸ ë¡œë“œ
        logger.info(f"ğŸ“¦ BGE-M3 ëª¨ë¸ ë¡œë”© ì‹œì‘: {BGE_MODEL_NAME}")
        bge_embedder = Embedder(model_name=BGE_MODEL_NAME)
        logger.info(f"âœ… BGE-M3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        logger.info(f"   ì°¨ì›: {bge_embedder.get_embedding_dimension()}")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

# ============================================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    
    # Sentence-Transformers ìƒíƒœ
    st_status = {
        "status": "healthy" if model is not None else "loading",
        "model_name": MODEL_NAME,
        "embedding_dimension": model.get_sentence_embedding_dimension() if model else None
    }
    
    # BGE-M3 ìƒíƒœ
    bge_status = {
        "status": "healthy" if bge_embedder is not None else "loading",
        "model_name": BGE_MODEL_NAME,
        "device": bge_embedder._device if bge_embedder else None,
        "embedding_dimension": bge_embedder.get_embedding_dimension() if bge_embedder else None
    }
    
    return {
        "service": "Multi-Model Embedding Service",
        "models": {
            "sentence_transformers": st_status,
            "bge_m3": bge_status
        }
    }

@app.post("/embed", response_model=EmbedResponse)
async def embed_texts(request: EmbedRequest):
    """
    Sentence-Transformers ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
    
    ê¸°ì¡´ sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
    """
    if model is None:
        raise HTTPException(
            status_code=503,
            detail="Sentence-Transformers ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    try:
        logger.debug(f"ğŸ“ [ST] ì„ë² ë”© ìƒì„± ìš”ì²­: {len(request.texts)}ê°œ í…ìŠ¤íŠ¸")
        
        # ì„ë² ë”© ìƒì„±
        embeddings = model.encode(
            request.texts,
            convert_to_numpy=True,
            normalize_embeddings=request.normalize,
            show_progress_bar=False
        )
        
        embeddings_list = embeddings.tolist()
        
        logger.debug(f"âœ… [ST] ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings_list)}ê°œ")
        
        return EmbedResponse(
            embeddings=embeddings_list,
            dimension=len(embeddings_list[0]) if embeddings_list else 0,
            count=len(embeddings_list)
        )
        
    except Exception as e:
        logger.error(f"âŒ [ST] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@app.post("/embed_bge_m3", response_model=EmbedResponse)
async def embed_texts_bge_m3(request: EmbedRequest):
    """
    BGE-M3 ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
    
    BAAI/bge-m3 ëª¨ë¸ ì‚¬ìš© (1024ì°¨ì›)
    Hugging Face Transformers ì§ì ‘ ì‚¬ìš©
    """
    if bge_embedder is None:
        raise HTTPException(
            status_code=503,
            detail="BGE-M3 ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    try:
        logger.debug(f"ğŸ“ [BGE-M3] ì„ë² ë”© ìƒì„± ìš”ì²­: {len(request.texts)}ê°œ í…ìŠ¤íŠ¸")
        
        # Embedder í´ë˜ìŠ¤ì˜ embed_batch ì‚¬ìš©
        embeddings_list = await bge_embedder.embed_batch(
            texts=request.texts,
            batch_size=32,
            normalize=request.normalize
        )
        
        logger.debug(f"âœ… [BGE-M3] ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings_list)}ê°œ")
        
        return EmbedResponse(
            embeddings=embeddings_list,
            dimension=len(embeddings_list[0]) if embeddings_list else 0,
            count=len(embeddings_list)
        )
        
    except Exception as e:
        logger.error(f"âŒ [BGE-M3] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@app.post("/similarity", response_model=SimilarityResponse)
async def calculate_similarity(request: SimilarityRequest):
    """
    ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì½”í¼ìŠ¤ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
    
    ê¸°ì¡´ Sentence-Transformers ëª¨ë¸ ì‚¬ìš©
    """
    if model is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    try:
        logger.debug(f"ğŸ” ìœ ì‚¬ë„ ê³„ì‚° ìš”ì²­: ì¿¼ë¦¬ 1ê°œ vs ì½”í¼ìŠ¤ {len(request.corpus_embeddings)}ê°œ")
        
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        query_embedding = model.encode(
            [request.query_text],
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        
        # ì½”í¼ìŠ¤ ì„ë² ë”©
        corpus_embeddings = np.array(request.corpus_embeddings)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(corpus_embeddings, query_embedding.T).flatten()
        similarities_list = similarities.tolist()
        
        logger.debug(f"âœ… ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {len(similarities_list)}ê°œ")
        
        return SimilarityResponse(
            similarities=similarities_list,
            count=len(similarities_list)
        )
        
    except Exception as e:
        logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - ì„œë¹„ìŠ¤ ì •ë³´ ì œê³µ"""
    return {
        "service": "Multi-Model Embedding Service",
        "version": "1.0.0",
        "models": {
            "sentence_transformers": {
                "name": MODEL_NAME,
                "status": "running" if model is not None else "loading"
            },
            "bge_m3": {
                "name": BGE_MODEL_NAME,
                "status": "running" if bge_embedder is not None else "loading"
            }
        },
        "endpoints": {
            "health": "GET /health - í—¬ìŠ¤ì²´í¬",
            "embed": "POST /embed - Sentence-Transformers ì„ë² ë”©",
            "embed_bge_m3": "POST /embed_bge_m3 - BGE-M3 ì„ë² ë”©",
            "similarity": "POST /similarity - ìœ ì‚¬ë„ ê³„ì‚°",
            "docs": "GET /docs - API ë¬¸ì„œ (Swagger UI)"
        }
    }


# ============================================================================
# ì‹¤í–‰ (ê°œë°œ í™˜ê²½)
# ============================================================================
if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )